{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "import requests\n",
    "from itertools import cycle\n",
    "from lxml.html import fromstring\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "tqdm.pandas()\n",
    "from datetime import datetime as dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your result path here\n",
    "collect_dir = \"./\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your token here, it can take multiple tokens\n",
    "bearer_tokens = [\n",
    "    'token1',\n",
    "    'token2',\n",
    "    'token3'\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create headers for API\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token),\n",
    "              \"Cache-Control\":\"no-cache\"}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error class\n",
    "class TooManyRequests(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "class ChangeBearerToken(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "class GeneralException(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)  \n",
    "class ServiceNotAvailable(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    global response\n",
    "    params['next_token'] = next_token\n",
    "    time.sleep(3)\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    if response.status_code != 200:\n",
    "        if \"TooManyRequests\" in response.text:\n",
    "            raise TooManyRequests(response.text)\n",
    "        elif \"UsageCapExceeded\" in response.text:\n",
    "            raise ChangeBearerToken(response.text)\n",
    "        elif \"Service Unavailable\" in response.text or response.status_code == 503:\n",
    "            raise ServiceNotAvailable(response.text)\n",
    "        else:\n",
    "            raise GeneralException(response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results will be saved as gz files in a monthly folder\n",
    "def append_to_gz(merged_dicts, fileName):\n",
    "\n",
    "    fout = gzip.open(fileName, \"a\")\n",
    "\n",
    "    try:\n",
    "        json_str = json.dumps(merged_dicts) + \"\\n\"            \n",
    "        json_bytes = json_str.encode('utf-8')           \n",
    "        fout.write(json_bytes)   \n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Tweets Written Failed\") \n",
    "        \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to request tweets\n",
    "def get_tweets(bearer_token, folder_name, fname, startdate, enddate):\n",
    "    global json_response;\n",
    "    global response \n",
    "    global merged_dicts;\n",
    "\n",
    "    headers = create_headers(bearer_token)\n",
    "    max_results = 500 # define the max results that one page can collect. Change to a smaller number will increase the speed for collecting a page data, but will result in needing more pages.\n",
    "    url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    \n",
    "    # change this query params based on your own needs; you can define keywords/location/time etc.\n",
    "    # for more information including limits, please read the official twitter api documents.\n",
    "    query_params = {\"query\": \"(asian OR Japanese OR thai OR korean OR asians OR filipino OR vietnamese OR nepal OR nigga OR niggas OR black (boy OR boys OR girl OR girls OR man OR men OR people OR ppl OR woman OR women) OR blm OR nigger OR negro OR blacks OR slavery OR #blacklivesmatter OR mexican OR hispanic OR latino OR latina OR mexicans OR puerto rican OR dominican OR cuban OR latinos OR border (security OR wall) OR latinx OR immigration OR immigrants OR illegals OR immigrant OR sanctuary OR migrants OR migrant OR undocumented OR illegal (aliens OR immigrants) OR afghanistan OR jewish OR taliban OR muslim OR jews OR yemeni OR muslims OR islam OR afghan OR racist OR racism OR racists OR poc OR minorities OR people of color OR interracial OR biracial OR sioux OR cherokee OR native (american OR americans) OR refugee OR whites OR nazis OR white (boy OR boys OR girl OR girls OR guy OR man OR men OR people OR ppl OR woman OR women) OR kkk OR redneck OR proud boys OR Caucasian OR black girl OR chinese) place_country:US has:geo lang:en\",\n",
    "                    'max_results': max_results,\n",
    "                    'start_time': startdate,\n",
    "                    'end_time': enddate,\n",
    "                    'expansions': 'geo.place_id,author_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id,entities.mentions.username',\n",
    "                    'tweet.fields': 'id,text,author_id,created_at,geo,lang,public_metrics,source,conversation_id,in_reply_to_user_id,referenced_tweets',\n",
    "                    'user.fields': 'id,name,username,created_at,description,location,pinned_tweet_id,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,geo,name,place_type',\n",
    "                    'next_token':{}}\n",
    "\n",
    "    total_tweets = 0\n",
    "    \n",
    "    flag = True\n",
    "    next_token = None\n",
    "    page = 0;\n",
    "    print(\"Start time: \" + str(startdate) + \" Current time: \" + str(dtime.now()))\n",
    "\n",
    "    while flag:\n",
    "        page =  page + 1;\n",
    "        json_response = connect_to_endpoint(url, headers, query_params, next_token)\n",
    "        \n",
    "        print(\"\\n Page #: \" + str(page), end=\". \")\n",
    "        print(\"Tweets Received: \" + str(len(json_response['data'])), end=\". \")\n",
    "        \n",
    "        append_to_gz(json_response, collect_dir + folder_name + '/'  + fname +\".gz\")\n",
    "        \n",
    "        while True:\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                next_token = json_response['meta']['next_token']\n",
    "            else:\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            break\n",
    "        \n",
    "        total_tweets = total_tweets + len(json_response['data'])\n",
    "         \n",
    "    print(\"Total number of results: \", str(total_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271cb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for the listed error exceptions here, the current page will be restarted, which means duplicate tweets may be collected. Remove duplicates during the post process.\n",
    "def do_everything(folder_name, fname, start, end, iii):\n",
    "\n",
    "    iii = iii\n",
    "    while True:\n",
    "            try:\n",
    "                if not os.path.isdir(collect_dir + folder_name):\n",
    "                    os.makedirs(collect_dir + folder_name)\n",
    "                get_tweets(bearer_tokens[iii], folder_name, fname, start, end)\n",
    "            except ChangeBearerToken as e: \n",
    "                print('Change bearer token: ' + str(e) + ' restart this page')\n",
    "                iii = iii + 1\n",
    "                if iii == len(bearer_tokens):\n",
    "                    iii = 0\n",
    "                    print(\"All tokens are used for current time period\")\n",
    "                continue\n",
    "            except GeneralException as e:\n",
    "                print('General exception: ' + str(e) + ' restart this page')\n",
    "                continue\n",
    "            except TooManyRequests as e:\n",
    "                print('Too many requests: ' + str(e) + ' restart this page')\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except ServiceNotAvailable as e:\n",
    "                print('Service not available: ' + str(e) + ' restart this page')\n",
    "                continue\n",
    "            break\n",
    "    return iii   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43929cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the timeframe you want to collect tweets from.\n",
    "date = pd.date_range(start='2022-01-01T00:00:00.000Z', periods=745, freq='H') \n",
    "date = [datetime.datetime.strftime(i, \"%Y-%m-%dT%H:00:00Z\") for i in date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main code\n",
    "iii = 0\n",
    "\n",
    "for i in range(len(date) - 1):\n",
    "        connected = False\n",
    "        while not connected:\n",
    "            try:\n",
    "                start_time = date[i];\n",
    "                end_time = date[i + 1];\n",
    "\n",
    "                iii = do_everything( start_time[:7], start_time[8:10], start_time, end_time, iii)\n",
    "                print(\"current bearer token is \" + str(iii))\n",
    "                connected = True\n",
    "            except Exception as e:\n",
    "                print('figure out why' + str(e) + \"*********************\")\n",
    "                \n",
    "                # date-hour that with issues will be saved to a file.                \n",
    "                with open (\"C:/Users/xyue/collected_data_full_academic/log_witherros.csv\",'a', newline='') as filedata:\n",
    "                    writer = csv.writer(filedata)\n",
    "                    row = [str(start_time)]\n",
    "                    writer.writerow(row)\n",
    "                filedata.close()\n",
    "                \n",
    "                connected = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
